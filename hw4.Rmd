---
title: "hw4"
author: "Maggie Meng"
date: "2021/11/17"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(igraph)
library(data.table)
library(reshape2)
library(PearsonDS)
library(dplyr)
library(data.table)

producers_and_films = fread("producers_and_films.csv", header = TRUE)
production_subsidiaries = fread("production_subsidiaries.csv", header = TRUE)
box_office_revenues = fread("box_office_revenues.csv", header = TRUE)
film_keywords = fread("film_keywords.csv", header = TRUE)
films_and_genres = fread("films_and_genres.csv", header = TRUE)
```

```{r}
film_data = producers_and_films[(producers_and_films$country == "us"),]
film_data[ , `:=`(count = .N) , by = prod_company]
test = film_data$prod_company
length(unique(test))
#film_data[ , `:=`(latest = max(year)) , by = prod_company]
```


## Question 1
Each year eigenvector
```{r}
mat = matrix(ncol = 0, nrow = 0)
empty_df = data.frame(mat)

for (i in 1985:2019) {
  film_data_2 = film_data[(film_data$year == i), c(2,3,5)]
  dt_1985 = as.data.frame.matrix(table(film_data_2[,c(3,2)]))
  #create each year affliation matrix
  dt_1985 = as.matrix(dt_1985)
  #create each year co-affliation  matrix
  co_affli_1985 = dt_1985 %*% t(dt_1985)
  
  
  adj_1985 = co_affli_1985
  #convert non-zero numbers in matrix to 1
  adj_1985[adj_1985 != 0] <- 1
  #convert diagnal numbers to 0
  diag(adj_1985) <- 0
  
  network_1985 = graph.adjacency(as.matrix(adj_1985))
  #calculate eigenvector centrality coreness
  eigen_1985 = unlist(centr_eigen(network_1985, directed = FALSE)$vector)
  #extract name of each eigenvector centrality coreness
  name_1985 = V(network_1985)$name
  
  final <- data.frame(matrix(nrow = length(eigen_1985), ncol = 3))
  colnames(final) <- c("year", "producer", "eigen")
  
  final[,2] <- name_1985
  final[,3] <- eigen_1985
  final[,1] <- i
  
  #row combine each year data frame to a complete data frame
  empty_df <- rbind(empty_df, final)
}
```

```{r}
library(dplyr)
library(tidyr)
library(zoo)

full_df = empty_df
#add 0 to missing year for each producer
full_df <- full_df %>% 
  group_by(producer) %>% 
  complete(year = min(year):max(year), fill = list(eigen = 0)) %>% 
  arrange(producer, year)

#calculate rolling avg for each producer
roll_avg_df <- full_df %>%
  mutate(RollAvge= lag(rollapplyr(eigen,10, mean, partial = TRUE)))

#convert NA values to 0
roll_avg_df[is.na(roll_avg_df$RollAvge),]$RollAvge <- 0

#find cutoff point for generalist and specialist
cutoff = quantile(roll_avg_df$RollAvge, probs = 0.75, na.rm = TRUE) #0.02152301,0.003959703 
cutoff

#assign producer type based on rolling avg
roll_avg_df$prod_type[roll_avg_df$RollAvge > cutoff] <- "generalist"
roll_avg_df$prod_type[roll_avg_df$RollAvge < cutoff] <- "specialist"
```

```{r}
#rearrange data frame
ref <- roll_avg_df[,c(1,2,5)]

colnames(ref)[1] <- "prod_company"
colnames(ref)[2] <- "year"

prod_data = film_data
#add year into data frame
prod_data <- merge(prod_data, ref, by = c("prod_company","year"), all = TRUE)

#delete NA rows
prod_data <- prod_data[is.na(prod_data$project) == FALSE,]
```

Classify each film by the type of collaboration

i. Peripheral solo productions: films made by a single specialist
ii. Central solo productions: films made by a single generalist
iii. Central co-productions: films made by a group of multiple generalists
iv. Peripheral co-productions: films made by a group of multiple specialists
v. Hybrid co-productions: films made by a group of generalists and specialists
```{r}
#count number of film per producer
prod_data[ , `:=`(count_film = .N) , by = project]
#count number of unique producer type
prod_data <- within(prod_data, { count_unique <- ave(prod_type, project, FUN=function(x) length(unique(x)))})

#assign type for each film 
film_type_data <- prod_data %>%
  group_by(project) %>%
  mutate(film_type = case_when(count_film == 1 & prod_type == "specialist" ~ "Peripheral solo productions",
                               count_film == 1 & prod_type == "generalist" ~ "Central solo productions",
                               count_film > 1 & count_unique == 1 & prod_type == "specialist" ~ "Peripheral co-productions",
                               count_film > 1 & count_unique == 1 & prod_type == "generalist" ~ "Central co-productions",
                               TRUE ~ "Hybrid co-productions"))
```

```{r}
film_year = unique(film_type_data[,c(2,3)])
keywords_data = merge(film_year, film_keywords, by = c("pindex"))
#keywords_data <- keywords_data[is.na(keywords_data$keyword) == FALSE,]

#sort the year
keywords_data_asc = keywords_data[order(keywords_data$keyword,keywords_data$year),]

#create new column of earliest year for each keyword
keywords_earliest <- keywords_data_asc %>% 
  group_by(keyword) %>%
  mutate(earlist_year = min(year))

#rearrange data frame
keywords_earliest2 = keywords_earliest[,c(1,2,4)]

#consider a keyword to be “new” if it has appeared for the first time within the last three years.
keywords_earliest <- keywords_earliest2 %>% 
  mutate(is_new = ifelse(keywords_earliest$year - keywords_earliest$earlist_year <= 2,1,0))

#get the number of new keyword for each film
keywords_earliest <- keywords_earliest %>% 
  group_by(pindex) %>%
  mutate(new_count = sum(is_new))

#rearrange data frame
keyword_is_new <- unique(keywords_earliest[,c(1,2,5)])
keyword_is_new[is.na(keyword_is_new)] <- 0

#merge with the full table
#keywords_final_1 <- unique(merge(film_type_data, keyword_is_new, by = "pindex"))
```

### A)
```{r}
film_type = unique(film_type_data[,c(3,11)])
#add film type into data frame
keyword_plot_1 = merge(film_type, keyword_is_new, by = c("pindex"))

keyword_plot_1 <- keyword_plot_1[,c(2,3,4)]

#find sum of new keywords per type in each year
keyword_plot_1 <- aggregate(new_count ~ film_type + year, data=keyword_plot_1, FUN=sum)

library(ggplot2)

ggplot(data = keyword_plot_1, aes(x = year, y = new_count, color=film_type, group = film_type)) +
  geom_line() +
  geom_point() + 
  ggtitle("Plot of never-before-seen keywords per type")
  
```


```{r}
# new_comb = merge(film_keywords, film_keywords, by = "pindex", allow.cartesian=TRUE)
# new_comb = merge(film_year, new_comb, by = c("pindex"))
# #keep only 2 different keywords
# new_comb <- new_comb[new_comb$keyword.x != new_comb$keyword.y,]
# new_comb[c(1:5),]
# 
# #create new column of earliest year for each keyword
# new_comb_2 <- new_comb %>%
#   group_by(keyword.x,keyword.y) %>%
#   mutate(earlist_year = min(year))
# new_comb_2[c(1:5),]
# 
# #rearrange data frame
# new_comb_3 = new_comb_2[,c(1,2,5)]
# new_comb_3[c(1:5),]
# 
# #consider a combination to be “new” if it has appeared for the first time within the last three years.
# new_comb_3 <- new_comb_3 %>%
#   mutate(is_new = ifelse(new_comb_3$year - new_comb_3$earlist_year <= 2,1,0))
# new_comb_3[c(1:5),]
# 
# new_comb_3 <- new_comb_3[,c(1,2,4)]
# 
# #change to data table
# new_comb_3 = data.table(new_comb_3)
# 
# #count the number of new combination for each film
# new_comb_count = new_comb_3[, .(new_count = sum(is_new)/2), by = pindex]
# new_comb_count[c(1:5),]
# 
# new_comb_count[is.na(new_comb_count)] <- 0
```

```{r}
#since it takes me half hour to run the above for loop, I save it as an RData document
load("new_comb_count.RData")
```


```{r}
keyword_plot_2 = merge(film_year, new_comb_count, by = c("pindex"))
keyword_plot_2 = merge(film_type, keyword_plot_2, by = c("pindex"))

keyword_plot_2 <- keyword_plot_2[,c(2,3,4)]

#find sum of new keywords per type in each year
keyword_plot_2 <- aggregate(new_count ~ film_type + year, data=keyword_plot_2, FUN=sum)

ggplot(data = keyword_plot_2, aes(x = year, y = new_count, color=film_type, group = film_type)) +
  geom_line() +
  geom_point() + 
  ggtitle("Plot of new combinations of existing keywords per type")
```


### B)
```{r}
prod_type_data = film_type_data[,c(1,2,3,4,5,11)]

#change film_type column to 3 binary columns 
dfcat = data.frame(model.matrix(~ prod_type_data$film_type-1, data=prod_type_data))
dfcat = dfcat[,c(1,3,4)]
colnames(dfcat)[1] <- "is_central_co-productions"
colnames(dfcat)[2] <- "is_hybrid_co-productions"
colnames(dfcat)[3] <- "is_peripheral_co-productions"

prod_type_data = cbind(prod_type_data, dfcat)

names(dfcat) = levels(prod_type_data$film_type)

#delete original column
prod_type_data$film_type <- NULL

box_office_revenues2 = box_office_revenues[,c(1,4)]
revenue_data = merge(prod_type_data, box_office_revenues2, by = c("pindex"), all = TRUE)

#delete NA rows
revenue_data <- revenue_data[is.na(revenue_data$project) == FALSE,]

#calculate operation year
operation_year_data <- revenue_data %>% 
    group_by(pcindex) %>% 
    mutate(min_year = min(year)) %>% 
    mutate(operation_year = (year - min_year + 1))

subsidiary_data = merge(operation_year_data, production_subsidiaries, by = c("pcindex"), all = TRUE)

#check na values
na_count <-sapply(subsidiary_data, function(y) sum(length(which(is.na(y)))))
data.frame(na_count)

#delete NA rows
subsidiary_data <- subsidiary_data[is.na(subsidiary_data$pindex) == FALSE,]

#find is subsidiary or not
subsidiary_data  <- subsidiary_data  %>%
  mutate(is_subsidiary = case_when(year >= first_year | year <= last_year ~ 1,
                                   TRUE ~ 0))

subsidiary_data = subsidiary_data[,-c(10,12,13)]
```

```{r}
library(proxy)

producer_film = unique(producers_and_films[,c(1,4)])
#add pcindex into dataframe
keywords_data_2 = merge(keywords_data, producer_film, by = c("pindex"), all = TRUE)

#delete NA rows
keywords_data_2 <- keywords_data_2[is.na(keywords_data_2$keyword) == FALSE,]
```


```{r}
# #create empty data frame
# coordinate = data.frame(mat)
# 
# #get coordinate for each producer in each year
# for (i in 1987:2019) {
#   year_df = keywords_data_2[(keywords_data_2$year <= i & keywords_data_2$year >= (i-2)), ]
#   
#   #create each year affliation matrix
#   aff = as.data.frame.matrix(table(year_df[,c(4,3)]))
#   aff = as.matrix(aff)
#   
#   #find jaccard distance
#   ja_dist <- dist(aff, method = "jaccard", by_rows = TRUE)
#   #multidimensional scaling
#   ja_dist <- cmdscale(ja_dist, k=2)
# 
#   ja_dist = data.frame(ja_dist)
#   
#   #make row names as the first column
#   final <- tibble::rownames_to_column(ja_dist, "VALUE")
#   colnames(final)[1] <- "pcindex" 
#   #add year as column
#   final[,4] <- i
#   colnames(final)[4] <- "year"
#   
#   #combine all the dataframe together
#   coordinate <- rbind(coordinate, final)
# }
# 
# #make row names as the first column
# #coordinate <- tibble::rownames_to_column(coordinate, "VALUE")
```


```{r}
#since it takes me half hour to run the above for loop, I save it as an RData document
load("coordinate.RData")
```

```{r}
#merge with previous table
q1_b = merge(subsidiary_data, coordinate, by = c("pcindex","year"), all = TRUE)
q1_b <- q1_b[is.na(q1_b$pindex) == FALSE,]

#add new key word and new combinations
q1_b_keyword = merge(q1_b, keyword_is_new, by = c("pindex","year"), all = TRUE)
colnames(q1_b_keyword)[14] <- "new_keyword"
q1_b_keyword_2 = merge(q1_b_keyword, new_comb_count, by = c("pindex"), all = TRUE)
colnames(q1_b_keyword_2)[15] <- "new_comb"

q1_b_keyword_2 = data.table(q1_b_keyword_2)
q1_b_keyword_2[ , `:=`(count_film = .N) , by = c("pcindex","year")]
```


```{r}
#find  there is keyword information or not
yes_keyword = film_keywords
yes_keyword$yes_keyword = 1
yes_keyword = unique(yes_keyword[,c(1,3)])

q1_b_keyword_3 = merge(q1_b_keyword_2, yes_keyword, by = c("pindex"), all = TRUE)

#offset
####q1_b_keyword_3[is.na(q1_b_keyword_3$yes_keyword)]$count_film <- 0

#offset
q1_b_final <- q1_b_keyword_3 %>% 
    group_by(pcindex,year) %>% 
    mutate(offset = sum(yes_keyword)) 
```

```{r}
library(MASS)
q1_b_2 <- q1_b_final[q1_b_final$year >= 1987,]

####q1_b_keyword_3[is.na(q1_b_keyword_3)] <- 0

####test <- data.frame(q1_b_keyword_3[complete.cases(q1_b_keyword_3),])

#new keyword
summary(glm.nb(new_keyword ~ `is_central_co-productions` + `is_hybrid_co-productions` + `is_peripheral_co-productions` + X1 + X2 + total_box + operation_year + is_subsidiary + factor(year), data = q1_b_2, offset(offset)))

#new combination
summary(glm.nb(new_comb ~ `is_central_co-productions` + `is_hybrid_co-productions` + `is_peripheral_co-productions` + X1 + X2 + total_box + operation_year + is_subsidiary + factor(year), data = q1_b_2, offset(offset)))
```

For the new keywords, the hybrid co-productions result in higher coefficient, which means they have more new keywords.
For the new combinations of existing keywords, it is also the hybrid co-productions result in higher coefficient, indicating they have more new keywords.


## Question 2
```{r}
# #create empty data frame
# avg_dist_final = data.frame(mat)
# 
# for (i in 1987:2019) {
#   year_df = keywords_data_2[(keywords_data_2$year <= i & keywords_data_2$year >= (i-2)), ]
# 
#   #create each year affiliation matrix
#   aff = as.data.frame.matrix(table(year_df[,c(4,3)]))
#   aff = as.matrix(aff)
# 
#   #find jaccard distance
#   ja_dist <- dist(aff, method = "jaccard", by_rows = TRUE)
#   ja_dist <- as.matrix(ja_dist)
# 
#   #create data frame of Jaccard distance between a producer and the other producers
#   dist_df = data.frame(prod_1=colnames(ja_dist)[col(ja_dist)], prod_2=rownames(ja_dist)[row(ja_dist)], dist=c(ja_dist))
#   #delete rows where producer 1 and producer 2 are the same
#   dist_df = dist_df[dist_df$prod_1 != dist_df$prod_2,]
# 
#   dist_df = data.table(dist_df)
# 
#   #find avg distance per producer
#   avg_dist_df = dist_df[, .(avg_dist = mean(dist)), by = prod_1]
# 
#   #add year as column
#   avg_dist_df = avg_dist_df[ , `:=` (year = i)]
# 
#   avg_dist_final <- rbind(avg_dist_final, avg_dist_df)
# }
```

```{r}
#since it takes me half hour to run the above for loop, I save it as an RData document
load("avg_dist_final.RData")
```

```{r}
colnames(avg_dist_final)[1] <- "pcindex"

q2 = merge(producer_film, keyword_is_new, by = c("pindex"), all = TRUE)

q2_2 = merge(q2, avg_dist_final, by = c("pcindex","year"), all = TRUE)

q2_2 <- q2_2[is.na(q2_2$pindex) == FALSE,]
q2_2 <- q2_2[is.na(q2_2$year) == FALSE,]
```

```{r}
q2_2_final <- q2_2[,c(4,5)]

#plot the distance between a producer and the other producers it works with relates to the number of new 
#keywords a producer introduces each year.
ggplot(q2_2_final, aes(avg_dist, new_count)) +
  geom_smooth(method = "loess", se = T) + 
  labs(x = "Average Jaccard distance", y = "New keywords")
```

The pattern shows that when Jaccard distance approaching to 0.96, the number of new keywords reach its highest point. This pattern suggest that when two dissimilar producers work together, they can producer more innovative idea. However, they cannot be dissimilar at all, which will cause an negative effect on the creation of new keywords.


## Question 3
```{r}
#extract screen release data
screen_release = box_office_revenues[,c(3,4)]

q3 = q1_b_keyword_3

#join screen release to original data frame
q3 = merge(q3, screen_release, by = c("pindex"), all = TRUE)

#delete rows where pcindex is NA
q3 = q3[is.na(q3$pcindex) == FALSE,]

q3 = data.table(q3)

#calculate return value for each producer per year
q3_return = q3[, `:=`(return = sum(total_box)/sum(release_coverage)), by = c("pcindex","year")]

q3_return = q3_return[is.na(q3_return$return) == FALSE,]
q3_return = q3_return[q3_return$return != Inf,]

#calculate mean return and sd return per year
q3_return = q3_return[, `:=`(mean_return = mean(return), sd_return = sd(return)), by = year]

q3_stand = q3_return[, `:=`(stand_return = (return - mean_return)/sd_return)]
```

```{r}
summary(lm(stand_return ~ `is_central_co-productions` + `is_hybrid_co-productions` + `is_peripheral_co-productions` + X1 + X2 + total_box + operation_year + is_subsidiary + factor(year), data = q3_stand))
```

The result shown above suggests that central co-productions partnership will bring a higher production companies' financial returns. The Peripheral co-productions and the hybrid co-productions may result in a  negative financial outcomes for collaborations.


## Question 4
### A)
```{r}
q4_a = q1_b_keyword_3

q4_prod_type = unique(prod_type_data[,c(3,6)])
q4_a = merge(q4_a, q4_prod_type, by = c("pindex"), all.x = TRUE)

#only select solo type
q4_a_1 = q4_a[(film_type == "Peripheral solo productions" | film_type == "Central solo productions"),]

q4_a_1 = data.table(q4_a_1)

#calculate the count of new keywords introduced in a producer’s solo produced films in a year
q4_a_solo <- q4_a_1[ , `:=`(solo_new_keword = sum(new_keyword)) , by = .(pcindex,year)]
q4_a_solo[is.na(q4_a_solo$solo_new_keword),]$solo_new_keword <- 0

q4_a_solo_final = unique(q4_a_solo[,c(1,2,3,19)])
```

```{r}
q4_a_2 = q4_a[film_type == "Hybrid co-productions",]

q4_a_2 = data.table(q4_a_2)

#the cumulative number of new keywords a producer has introduced in all of its films through the current year
q4_a_hybird <- q4_a_2[ , `:=`(hybird_new_keword = cumsum(new_keyword)) , by = .(pcindex,year)]
q4_a_hybird[is.na(q4_a_hybird$hybird_new_keword),]$hybird_new_keword <- 0

#rearrange the table
q4_a_hybird_final = unique(q4_a_hybird[,c(1,2,3,19)])
```

```{r}
q4_a_final = q1_b_keyword_3
q4_a_final = merge(q4_a, q4_a_solo_final, by = c("pcindex","year","pindex"), all = TRUE)
q4_a_final = merge(q4_a_final, q4_a_hybird_final, by = c("pcindex","year","pindex"), all = TRUE)

#offset
q4_a_final_2 <- q4_a_final %>% 
    group_by(pcindex,year) %>% 
    mutate(offset = sum(yes_keyword)) 

q4_a_final_2 <- q4_a_final_2[q4_a_final_2$year >= 1987,]
```

```{r}
summary(glm.nb(new_keyword ~ `is_hybrid_co-productions` + X1 + X2 + total_box + operation_year + is_subsidiary + factor(year), data = q4_a_final_2, offset(offset)))
```

From the result shown above, we can see that creative innovation gained through collaborations does make a producer’s solo-produced films more innovative. It suggests that solo-produced films should involve in more collaboration in order to gain more new ideas.


### B)
```{r}
summary(lm(stand_return ~ `is_central_co-productions` + `is_hybrid_co-productions` + `is_peripheral_co-productions` + X1 + X2 + total_box + operation_year + is_subsidiary + factor(year) + new_keyword, data = q3_stand))
```

According to the result above, we can see that when a producer’s engaging in collaborations, introducing new keywords do result in higher box office returns. Thus, it makes sense that even though it can be financially risky, producers still want to engage in collaborations.


